{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa1nEHWmrcWX",
        "outputId": "3e9fdbfd-c43d-444a-e729-2ab2a50769c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import TreebankWordTokenizer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import urllib\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the data\n",
        "def print_some_url():\n",
        "    with urllib.request.urlopen('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint//alice_in_wonderland.txt') as f:\n",
        "        return f.read().decode('ISO-8859-1')\n",
        "\n",
        "data = print_some_url()\n",
        "print(data[:863])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdO1-zoLt7qF",
        "outputId": "7d7392f4-bceb-441a-a0c3-5d10d28b35f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice's Adventures in Wonderland\n",
            "\n",
            "                ALICE'S ADVENTURES IN WONDERLAND\n",
            "\n",
            "                          Lewis Carroll\n",
            "\n",
            "               THE MILLENNIUM FULCRUM EDITION 3.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            CHAPTER I\n",
            "\n",
            "                      Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "  Alice was beginning to get very tired of sitting by her sister\n",
            "on the bank, and of having nothing to do:  once or twice she had\n",
            "peeped into the book her sister was reading, but it had no\n",
            "pictures or conversations in it, `and what is the use of a book,'\n",
            "thought Alice `without pictures or conversation?'\n",
            "\n",
            "  So she was considering in her own mind (as well as she could,\n",
            "for the hot day made her feel very sleepy and stupid), whether\n",
            "the pleasure of making a daisy-chain would be worth the trouble\n",
            "of getting up and picking the daisies, when suddenly a White\n",
            "Rabbit with pink eyes ran close by her.\n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to lowercase and remove punctuation\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    words = words.lower()\n",
        "    return ''.join([x for x in words if x not in string.punctuation])"
      ],
      "metadata": {
        "id": "Xeoe9K2TuFbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply the remove_punctuation function to the data\n",
        "data = remove_punctuation(data)"
      ],
      "metadata": {
        "id": "JuA0-KAxuW3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bag-of-words and assign our stemmer and lemmatizer\n",
        "\n",
        "# Define stemmer function\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Tokenise data\n",
        "tokeniser = TreebankWordTokenizer()\n",
        "tokens = tokeniser.tokenize(data)\n",
        "\n",
        "# Define lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Bag-of-words\n",
        "def bag_of_words_count(words, word_dict={}):\n",
        "    \"\"\" this function takes in a list of words and returns a dictionary\n",
        "        with each word as a key, and the value represents the number of\n",
        "        times that word appeared\"\"\"\n",
        "    for word in words:\n",
        "        if word in word_dict.keys():\n",
        "            word_dict[word] += 1\n",
        "        else:\n",
        "            word_dict[word] = 1\n",
        "    return word_dict\n",
        "\n",
        "# Remove stop words\n",
        "tokens_less_stopwords = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "# Create bag-of-words\n",
        "bag_of_words = bag_of_words_count(tokens_less_stopwords,{})"
      ],
      "metadata": {
        "id": "fBV9LqMpurBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that finds the stem and lemma of the nth word in the token list\n",
        "def find_roots(token_list, n):\n",
        "    root_dict = {}\n",
        "    word = token_list[n-1]\n",
        "    root_dict['original'] = word\n",
        "    root_dict[\"stem\"] = stemmer.stem(word)\n",
        "    root_dict[\"lemma\"] = lemmatizer.lemmatize(word)\n",
        "\n",
        "    return root_dict"
      ],
      "metadata": {
        "id": "lY_E4elEuw4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_roots(tokens, 120)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9b5BfTCv_u5",
        "outputId": "bcefbcad-2456-431f-e7bb-8758979df9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original': 'daisies', 'stem': 'daisi', 'lemma': 'daisy'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function that calculates the number of stop words that are in the text in total, including repetitions.\n",
        "def count_stopwords(token_list):\n",
        "    STOPwords = [word for word in token_list if word in stopwords.words(\"english\")]\n",
        "    return len(STOPwords)"
      ],
      "metadata": {
        "id": "grFTaFslwb3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_stopwords(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knfI8tq1yIlG",
        "outputId": "801b09ec-afed-4684-bd4a-2af61911a183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13774"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function that calculates the number of unique words in the text.\n",
        "def unique_words(token_list):\n",
        "    return len(set(token_list))"
      ],
      "metadata": {
        "id": "nTk5Hd2IxWAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKwl26vryYZz",
        "outputId": "162dd78c-9353-45bf-ca0c-bb8000e5fe71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2749"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(bag_of_words_count(tokens,{}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGaLdvpIyd4c",
        "outputId": "5de44ca2-5071-401b-c479-ad7f2ac18504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2749"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function that calculates the kth most frequently occurring word in the bag-of-words\n",
        "def most_common_word(bag, k):\n",
        "    switch = [(value, key) for key, value in bag.items()]\n",
        "    switch = sorted(switch)\n",
        "    return switch[-k][1]"
      ],
      "metadata": {
        "id": "wAsILZHNxavh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_common_word(bag_of_words, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n3IGTTBDy3wk",
        "outputId": "17f1ca9d-2b19-469c-c48e-299fd00de71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'little'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function that calculates the number of words that appear n times in the text.\n",
        "def word_frequency_count(bag, n):\n",
        "    total = sum(1 for value in bag.values() if value == n)\n",
        "    return total\n",
        "\n",
        "new_var = word_frequency_count(bag_of_words, 8)\n",
        "\n",
        "new_var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5YrtRXXx45v",
        "outputId": "9114fe25-f58c-4926-f762-6d1f7a5af9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}